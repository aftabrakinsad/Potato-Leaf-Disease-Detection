# -*- coding: utf-8 -*-
"""Ablation Study-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pQAbqq3UaHhENNY3olmCQhsHJQC5ikxl
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib as plt
import matplotlib.pyplot as plt
from PIL import Image
from IPython.display import HTML
from urllib.request import urlopen
from tensorflow.keras import models, layers
from sklearn.metrics import confusion_matrix
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/Potato/Potato_Dataset.zip

for dirname, _, filenames in os.walk('/content/Potato_Dataset'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

BATCH_SIZE = 64
IMAGE_SIZE = 240
CHANNELS=3
EPOCHS=40

dataset = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/Potato_Dataset",
    seed=123,
    shuffle=True,
    image_size=(IMAGE_SIZE,IMAGE_SIZE),
    batch_size=BATCH_SIZE
)

class_names = dataset.class_names
class_names

for image_batch, label_batch in dataset.take(1):
    print(image_batch.shape)
    print(label_batch.numpy())

    for image_batch, label_batch in dataset.take(1):
     print(image_batch[0].numpy())

plt.figure(figsize=(8, 9))
for image_batch, labels_batch in dataset.take(1):
    for i in range(15):
        ax = plt.subplot(5, 5, i + 1)
        plt.imshow(image_batch[i].numpy().astype("uint8"))
        plt.title(class_names[labels_batch[i]], fontweight='bold')
        plt.axis("off")

len(dataset)

train_size = 0.8
len(dataset)*train_size

train_ds = dataset.take(100)
len(train_ds)

test_ds = dataset.skip(100)
len(test_ds)

val_size=0.1
len(dataset)*val_size

val_ds = test_ds.take(12)
len(val_ds)

test_ds = test_ds.skip(12)
len(test_ds)

def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):
    assert (train_split + test_split + val_split) == 1

    ds_size = len(ds)

    if shuffle:
        ds = ds.shuffle(shuffle_size, seed=12)

    train_size = int(train_split * ds_size)
    val_size = int(val_split * ds_size)

    train_ds = ds.take(train_size)
    val_ds = ds.skip(train_size).take(val_size)
    test_ds = ds.skip(train_size).skip(val_size)

    return train_ds, val_ds, test_ds

train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)

len(train_ds)

len(val_ds)

len(test_ds)

train_ds = train_ds.cache().shuffle(2000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.cache().shuffle(2000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = test_ds.cache().shuffle(2000).prefetch(buffer_size=tf.data.AUTOTUNE)

resize_and_rescale = tf.keras.Sequential([
  layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),
  layers.Rescaling(1./255),
])

data_augmentation = tf.keras.Sequential([
  layers.RandomFlip("vertical_and_horizontal"),
  layers.RandomRotation(0.3),
])

train_ds = train_ds.map(
    lambda x, y: (data_augmentation(x, training=True), y)
).prefetch(buffer_size=tf.data.AUTOTUNE)

input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)
n_classes = 6

model = models.Sequential([
    resize_and_rescale,
    layers.Conv2D(64,  kernel_size = (3,3), activation='relu', input_shape=input_shape),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64,  kernel_size = (3,3), activation='relu',  input_shape=input_shape),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64,  kernel_size = (3,3), activation='relu',  input_shape=input_shape),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64,  kernel_size = (3,3), activation='relu',  input_shape=input_shape),
    layers.MaxPooling2D((2, 2)),

    layers.Conv2D(64,  kernel_size = (3,3), activation='relu',  input_shape=input_shape),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),

    layers.Dense(64, activation='relu'),
    layers.Dense(n_classes, activation='softmax'),
])

model.build(input_shape=input_shape)

model.summary()

model.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)
history = model.fit(
    train_ds,
    batch_size=BATCH_SIZE,
    validation_data=val_ds,
    verbose=1,
    epochs=80,
)

scores = model.evaluate(test_ds)

scores = model.evaluate(test_ds)
print(f"Test accuracy: {scores[1]*100:.2f}%")
print(f"Test loss: {scores[0]:.4f}")

model.save('potato_model.h5')

model = tf.keras.models.load_model('/content/potato_model.h5')

history
history.params
history.history.keys()

type(history.history['loss'])
len(history.history['loss'])

history.history['accuracy'][:5]

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(16, 4))
plt.subplot(1, 2, 1)
epochs_range = range(1, len(acc) + 1)
plt.plot(epochs_range, acc, label='Training Accuracy', color='#e60049')
plt.plot(epochs_range, val_acc, label='Validation Accuracy', color='#00bfa0')
plt.legend(loc='lower right')
#plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs', fontweight='bold', fontsize=12)
plt.ylabel('Accuracy', fontweight='bold', fontsize=12)
plt.show()

plt.figure(figsize=(16, 4))
plt.subplot(1, 2, 1)
epochs_range = range(1, len(loss) + 1)
plt.plot(epochs_range, loss, label='Training Loss', color='#e60049')
plt.plot(epochs_range, val_loss, label='Validation Loss', color='#00bfa0')
plt.legend(loc='upper right')
#plt.title('Training and Validation Loss')
plt.xlabel('Epochs', fontweight='bold', fontsize=12)
plt.ylabel('Loss', fontweight='bold', fontsize=12)
plt.show()

for images_batch, labels_batch in test_ds.take(1):

    first_image = images_batch[0].numpy().astype('uint8')
    first_label = labels_batch[0].numpy()

    print("First image to predict")
    plt.imshow(first_image)
    print("Actual label:", class_names[first_label])

    batch_prediction = model.predict(images_batch)
    print("Predicted label:", class_names[np.argmax(batch_prediction[0])])

def predict(model, img):
    img_array = tf.keras.preprocessing.image.img_to_array(img)
    img_array = tf.expand_dims(img_array, 0)

    predictions = model.predict(img_array)

    predicted_class = class_names[np.argmax(predictions[0])]
    confidence = round(100 * np.max(predictions[0]), 2)
    return predicted_class, confidence

actual_classes = []
predicted_classes = []

plt.figure(figsize=(15, 15))
for images, labels in test_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))

        predicted_class, confidence = predict(model, images[i].numpy())
        actual_class = class_names[labels[i]]
        actual_classes.append(actual_class)
        predicted_classes.append(predicted_class)

        plt.title(f"Actual: {actual_class},\n Predicted: {predicted_class}.\n Confidence: {confidence}%")

        plt.axis("off")

print("Actual Classes:", actual_classes)
print("Predicted Classes:", predicted_classes)

Y_true = []
Y_pred = []

for j in range(len(test_ds)):
    for images, labels in test_ds.take(j):
        for i in range(len(labels)):

            predicted_class, confidence = predict(model, images[i].numpy())
            actual_class = class_names[labels[i]]
            Y_true.append(actual_class)
            Y_pred.append(predicted_class)

cm = confusion_matrix(Y_true, Y_pred)
plt.figure(figsize=(7, 7))
ax = sns.heatmap(cm, cmap=plt.cm.Blues, annot=True, square=True, xticklabels=class_names, yticklabels=class_names)
ax.set_ylabel('Actual', fontweight='bold', fontsize=12)
ax.set_xlabel('Predicted', fontweight='bold', fontsize=12)

im_file = urlopen("https://i.ibb.co/hg1PrSd/late-bright.jpg")
image_file = Image.open(im_file)
plt.figure(figsize=(15, 15))

i = 0

ax = plt.subplot(3, 3, i + 1)
plt.imshow(image_file)
predicted_class, confidence = predict(model, np.array(image_file))
plt.title(f"Predicted: {predicted_class}.\n Confidence: {confidence}%")
plt.axis("off")
plt.show()

im_file = urlopen("https://i.ibb.co/nDf3TK1/early-bright.jpg")
image_file = Image.open(im_file)
plt.figure(figsize=(15, 15))

i = 0

ax = plt.subplot(3, 3, i + 1)
plt.imshow(image_file)
predicted_class, confidence = predict(model, np.array(image_file))
plt.title(f"Predicted: {predicted_class}.\n Confidence: {confidence}%")
plt.axis("off")
plt.show()

im_file = urlopen("https://i.ibb.co/9wMbr7d/pest.jpg")
image_file = Image.open(im_file)
plt.figure(figsize=(15, 15))

i = 0

ax = plt.subplot(3, 3, i + 1)
plt.imshow(image_file)
predicted_class, confidence = predict(model, np.array(image_file))
plt.title(f"Predicted: {predicted_class}.\n Confidence: {confidence}%")
plt.axis("off")
plt.show()

im_file = urlopen("https://i.ibb.co/pnK5wCx/healthy.jpg")
image_file = Image.open(im_file)
plt.figure(figsize=(15, 15))

i = 0

ax = plt.subplot(3, 3, i + 1)
plt.imshow(image_file)
predicted_class, confidence = predict(model, np.array(image_file))
plt.title(f"Predicted: {predicted_class}.\n Confidence: {confidence}%")
plt.axis("off")
plt.show()

im_file = urlopen("https://i.ibb.co.com/n35r36V/virus.jpg")
image_file = Image.open(im_file)
plt.figure(figsize=(15, 15))

i = 0

ax = plt.subplot(3, 3, i + 1)
plt.imshow(image_file)
predicted_class, confidence = predict(model, np.array(image_file))
plt.title(f"Predicted: {predicted_class}.\n Confidence: {confidence}%")
plt.axis("off")
plt.show()